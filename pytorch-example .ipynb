{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pygame\n",
    "from datetime import datetime\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "# from tf_agents.environments import tf_environment\n",
    "# from tf_agents.environments import tf_py_environment\n",
    "# from tf_agents.environments import utils\n",
    "# from tf_agents.specs import array_spec\n",
    "# from tf_agents.environments import wrappers\n",
    "# from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "# tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "# from simple_dqn_torch_2020 import Agent\n",
    "# from utils import plotLearning\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CopyEnv<Copy-v0>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class CardGameEnv(py_environment.PyEnvironment):\n",
    "\n",
    "#   def __init__(self):\n",
    "#     self._action_spec = array_spec.BoundedArraySpec(\n",
    "#         shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "#     self._observation_spec = array_spec.BoundedArraySpec(\n",
    "#         shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "#     self._state = 0\n",
    "#     self._episode_ended = False\n",
    "\n",
    "#   def action_spec(self):\n",
    "#     return self._action_spec\n",
    "\n",
    "#   def observation_spec(self):\n",
    "#     return self._observation_spec\n",
    "\n",
    "#   def _reset(self):\n",
    "#     self._state = 0\n",
    "#     self._episode_ended = False\n",
    "#     return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "#   def _step(self, action):\n",
    "\n",
    "#     if self._episode_ended:\n",
    "#       # The last action ended the episode. Ignore the current action and start\n",
    "#       # a new episode.\n",
    "#       return self.reset()\n",
    "\n",
    "#     # Make sure episodes don't go on forever.\n",
    "#     if action == 1:\n",
    "#       self._episode_ended = True\n",
    "#     elif action == 0:\n",
    "#       new_card = np.random.randint(1, 11)\n",
    "#       self._state += new_card\n",
    "#     else:\n",
    "#       raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "#     if self._episode_ended or self._state >= 21:\n",
    "#       reward = self._state - 21 if self._state <= 21 else -21\n",
    "#       return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "#     else:\n",
    "#       return ts.transition(\n",
    "#           np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)\n",
    "# env = gym.make('LunarLander-v2')\n",
    "env = gym.make('Copy-v0')\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up game area layout\n",
    "# By changing the block_size other aspects will scale\n",
    "block_size = 30    # Size of side of square\n",
    "blocks_w = 10      # game width (in blocks)\n",
    "blocks_h = 20      # game height (in blocks)\n",
    "border_w = 500\n",
    "border_h = 100\n",
    "end_wait = 2000\n",
    "\n",
    "play_w = blocks_w * block_size  # Width is 10 blocks\n",
    "play_h = blocks_h * block_size  # Height is 20 blocks\n",
    "full_w = 2 * border_w + play_w\n",
    "full_h = 2 * border_h + play_h + 50\n",
    "top_left_x = border_w\n",
    "top_left_y = 2 * border_h\n",
    "start_x = int(blocks_w / 2)\n",
    "\n",
    "NO_KEY = 0\n",
    "LEFT_KEY = 1\n",
    "RIGHT_KEY = 2\n",
    "DOWN_KEY = 3\n",
    "UP_KEY = 4\n",
    "\n",
    "\n",
    "I = [['..0..',\n",
    "      '..0..',\n",
    "      '..0..',\n",
    "      '..0..',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '0000.',\n",
    "      '.....',\n",
    "      '.....',\n",
    "      '.....']]\n",
    "#J\n",
    "J = [['.....',\n",
    "      '.0...',\n",
    "      '.000.',\n",
    "      '.....',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '..00.',\n",
    "      '..0..',\n",
    "      '..0..',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '.....',\n",
    "      '.000.',\n",
    "      '...0.',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '..0..',\n",
    "      '..0..',\n",
    "      '.00..',\n",
    "      '.....']]\n",
    "#L\n",
    "L = [['.....',\n",
    "      '.....',\n",
    "      '.000.',\n",
    "      '.0...',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '.00..',\n",
    "      '..0..',\n",
    "      '..0..',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '...0.',\n",
    "      '.000.',\n",
    "      '.....',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '..0..',\n",
    "      '..0..',\n",
    "      '..00.',\n",
    "      '.....']]\n",
    "#O\n",
    "O = [['.....',\n",
    "      '.....',\n",
    "      '.00..',\n",
    "      '.00..',\n",
    "      '.....']]\n",
    "#S\n",
    "S = [['.....',\n",
    "      '..0..',\n",
    "      '..00.',\n",
    "      '...0.',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '.....',\n",
    "      '..00.',\n",
    "      '.00..',\n",
    "      '.....']]\n",
    "#T\n",
    "T = [['.....',\n",
    "      '..0..',\n",
    "      '.000.',\n",
    "      '.....',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '..0..',\n",
    "      '..00.',\n",
    "      '..0..',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '.....',\n",
    "      '.000.',\n",
    "      '..0..',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '..0..',\n",
    "      '.00..',\n",
    "      '..0..',\n",
    "      '.....']]\n",
    "#Z\n",
    "Z = [['.....',\n",
    "      '.....',\n",
    "      '.00..',\n",
    "      '..00.',\n",
    "      '.....'],\n",
    "     ['.....',\n",
    "      '..0..',\n",
    "      '.00..',\n",
    "      '.0...',\n",
    "      '.....']]\n",
    "shapes = [I, J, L, O, S, T, Z]\n",
    "shapes_color = [(255,0,0),(0,255,0),\n",
    "                (0,0,255),(0,255,255),\n",
    "                (255,0,255),(255,255,0),\n",
    "                (128,0,128)]\n",
    "pygame.font.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Piece(object):\n",
    "    def __init__ (self, x, y, shape):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.shape = shape\n",
    "        self.color = shapes_color[shapes.index(shape)]\n",
    "        self.rotation = int(np.random.rand()*len(shape))\n",
    "\n",
    "def create_grid(locked_pos = {}):\n",
    "    grid = [[(0,0,0) for _ in range(blocks_w)] for _ in range(blocks_h)]\n",
    "    for hgt in range(blocks_h):\n",
    "        for wid in range(blocks_w):\n",
    "            if (wid, hgt) in locked_pos:\n",
    "                clr = locked_pos[(wid,hgt)]\n",
    "                grid[hgt][wid] = clr\n",
    "    return grid\n",
    "\n",
    "def convert_shape_fmt(this_shape):\n",
    "    posit = []\n",
    "    fmt = this_shape.shape [this_shape.rotation % len(this_shape.shape)]\n",
    "    for i, line in enumerate(fmt):\n",
    "        row = list(line)\n",
    "        for j, col in enumerate(row):\n",
    "            if col == '0':\n",
    "                posit.append((int(this_shape.x + j), int(this_shape.y + i)))\n",
    "                             \n",
    "    for i, pos in enumerate(posit):\n",
    "        posit[i] = (int(pos[0]-2), int(pos[1]-4))\n",
    "    return posit\n",
    "\n",
    "def valid_space(grid, piece):\n",
    "    accepted_pos = [[(j,i) for j in range (blocks_w) if grid[i][j] == (0,0,0)] for i in range (blocks_h)]\n",
    "    accepted_pos = [j for sub in accepted_pos for j in sub]\n",
    "    formatted = convert_shape_fmt(piece)\n",
    "    for pos in formatted:\n",
    "        if pos not in accepted_pos:\n",
    "            if pos[1] > -1:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def check_lost(positions):\n",
    "    for pos in positions:\n",
    "        x, y = pos\n",
    "        if y < -1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_shape():\n",
    "    return Piece(start_x, 0, np.random.choice(shapes))\n",
    "\n",
    "def draw_text_middle(surface, text, size, color):\n",
    "    font = pygame.font.SysFont('Arial',size, bold = True)\n",
    "    bolded = font.render(text, 1, color)\n",
    "    surface.blit(bolded,((full_w-bolded.get_width())/2, (full_h-bolded.get_height())/2))\n",
    "\n",
    "def draw_grid(surface, grid):\n",
    "    for hgt in range(blocks_h+1):\n",
    "        pygame.draw.line(surface,(128,128,128), \n",
    "                         (top_left_x, top_left_y + hgt*block_size), \n",
    "                         (top_left_x + blocks_w*block_size, top_left_y + hgt*block_size))\n",
    "    for wid in range(blocks_w+1):\n",
    "        pygame.draw.line(surface,(128,128,128), \n",
    "                         (top_left_x + wid*block_size, top_left_y), \n",
    "                         (top_left_x + wid*block_size, top_left_y + blocks_h*block_size))\n",
    "\n",
    "\n",
    "def clear_rows(grid, locked):\n",
    "\n",
    "    inc = 0\n",
    "    # backward (bottom to top scan)\n",
    "    for i in range(len(grid)-1, -1, -1):\n",
    "        row = grid[i]\n",
    "        if (0, 0, 0) not in row:\n",
    "            inc += 1\n",
    "            for j in range(len(row)):\n",
    "                del locked[(j,i)]\n",
    "        else:\n",
    "            if inc > 0:\n",
    "                k = i+inc\n",
    "                for j in range(len(row)):\n",
    "                    if (j,i) in locked:\n",
    "                        locked[(j,k)] = locked[(j,i)]\n",
    "                        del locked[(j,i)]\n",
    "\n",
    "    return inc, locked\n",
    "\n",
    "def draw_next_shape(surface, shp):\n",
    "    font = pygame.font.SysFont('Arial',60)\n",
    "    next_s = font.render('Next Shape:',1, (255,255,255))\n",
    "    next_x = top_left_x + play_w + 150\n",
    "    next_y = top_left_y + play_h/2 - 150\n",
    "    fmt = shp.shape[shp.rotation]\n",
    "    \n",
    "    for i, line in enumerate(fmt):\n",
    "        row = list(line)\n",
    "        for j,col in enumerate(row):\n",
    "            if col == '0':\n",
    "                pygame.draw.rect(surface, shp.color, (next_x + j*block_size, \n",
    "                                 next_y + i*block_size, \n",
    "                                 block_size, block_size), 0)\n",
    "    surface.blit(next_s, (next_x - 50, next_y - 100))\n",
    "\n",
    "def add_score(score):\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\\n\")\n",
    "    with open('data/score_log','a') as f:\n",
    "        f.writelines(str(int(score))+','+dt_string)\n",
    "    return score\n",
    "\n",
    "def draw_window(surface, grid, score):\n",
    "    surface.fill((0,0,0))\n",
    "    pygame.font.init()\n",
    "    font = pygame.font.SysFont('Arial',60)\n",
    "    title = font.render('Tetris',1, (255,255,255))\n",
    "    surface.blit(title,(full_w/2 - title.get_width()/2, 20))\n",
    "    \n",
    "    font = pygame.font.SysFont('Arial',60)\n",
    "    score_s = font.render('Score:',1, (255,255,255))\n",
    "    score_x = 50\n",
    "    score_y = top_left_y + 50\n",
    "    surface.blit(score_s, (score_x, score_y))\n",
    "    score_s = font.render(str(score),1, (255,255,255))\n",
    "    score_x = 100\n",
    "    score_y += 100\n",
    "    surface.blit(score_s, (score_x, score_y))\n",
    "    \n",
    "    for hgt in range(blocks_h):\n",
    "        for wid in range(blocks_w):\n",
    "            pygame.draw.rect(surface, grid[hgt][wid],\n",
    "                             (top_left_x + wid*block_size, \n",
    "                            top_left_y + hgt*block_size, \n",
    "                            block_size, block_size), 0)\n",
    "    pygame.draw.rect(surface, (255,0,0), (top_left_x, top_left_y, play_w, play_h), 4)\n",
    "    draw_grid(surface, grid)\n",
    "#     pygame.display.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tetris(py_environment.PyEnvironment):\n",
    "\n",
    "    def __init__(self):\n",
    "#         self._action_spec = array_spec.BoundedArraySpec(\n",
    "#             shape=(), dtype=np.int32, minimum=0, maximum=4, name='action')\n",
    "#         self._observation_spec = array_spec.BoundedArraySpec(\n",
    "#             shape=(blocks_h,blocks_w), dtype=np.int32, minimum=0, name='observation')\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        self.win = pygame.display.set_mode((full_w, full_h))\n",
    "        pygame.display.set_caption('Tetris')\n",
    "        self.win.fill((0, 0, 0))\n",
    "#         draw_text_middle(win,\"Press any key to Start\", 60, (255, 255, 255))\n",
    "        self.locked_blocks = {}\n",
    "        self.grid = create_grid(self.locked_blocks)\n",
    "\n",
    "        self.change_piece = False\n",
    "#     run = True\n",
    "        self.next_piece = get_shape()\n",
    "        self.curr_piece = get_shape()\n",
    "        self.score = 0\n",
    "        pygame.display.update()\n",
    "    \n",
    "#     clock = pygame.time.Clock()\n",
    "#     fall_time = 0\n",
    "#     fall_speed =.27\n",
    "#     level_time = 0\n",
    "#     score = 0\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self.grid\n",
    "#         return self._observation_spec\n",
    "\n",
    "    def time_step_spec(self):\n",
    "        \"\"\"Return time_step_spec.\"\"\"\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "#     if self._episode_ended:\n",
    "#       # The last action ended the episode. Ignore the current action and start\n",
    "#       # a new episode.\n",
    "#       TETRIS BOARD CLOGGED\n",
    "#       return self.reset()\n",
    "\n",
    "#     # Make sure episodes don't go on forever.     \n",
    "#     TETRIS GAME WILL END\n",
    "#     if action == 1:\n",
    "#       self._episode_ended = True\n",
    "#     elif action == 0:\n",
    "#       new_card = np.random.randint(1, 11)\n",
    "#       self._state += new_card\n",
    "#     else:\n",
    "        if action < 0 or action > 4:\n",
    "            raise ValueError('`action` should be between 0 and 4.')\n",
    "        self.curr_piece.y += 1\n",
    "        self.grid = create_grid(self.locked_blocks)\n",
    "        if not (valid_space(self.grid, self.curr_piece)) and (self.curr_piece.y > 0):\n",
    "            self.curr_piece.y -= 1\n",
    "            self.change_piece = True\n",
    "        \n",
    "        self.grid = create_grid(self.locked_blocks)\n",
    "        key_choice = action #(self.grid, self.curr_piece)\n",
    "        if key_choice == LEFT_KEY:\n",
    "            self.curr_piece.x -= 1\n",
    "            if not (valid_space(self.grid, self.curr_piece)):\n",
    "                self.curr_piece.x += 1\n",
    "        if key_choice == RIGHT_KEY:\n",
    "            self.curr_piece.x += 1\n",
    "            if not (valid_space(self.grid, self.curr_piece)):\n",
    "                self.curr_piece.x -= 1\n",
    "        if key_choice == DOWN_KEY:\n",
    "            while valid_space(self.grid, self.curr_piece):\n",
    "                self.curr_piece.y += 1\n",
    "            self.curr_piece.y -= 1\n",
    "        if key_choice == UP_KEY:\n",
    "            self.curr_piece.rotation += 1 \n",
    "            self.curr_piece.rotation %= len(self.curr_piece.shape)\n",
    "            if not (valid_space(self.grid, self.curr_piece)):\n",
    "                if self.curr_piece.rotation == 0:\n",
    "                    self.curr_piece.rotation = len(self.curr_piece.shape) - 1\n",
    "                else:\n",
    "                    self.curr_piece.rotation -= 1\n",
    "        # if none of the above, piece drops one row\n",
    "\n",
    "        shape_pos = convert_shape_fmt(self.curr_piece)\n",
    "        for i in range(len(shape_pos)):\n",
    "            x, y = shape_pos[i]\n",
    "            if y > -1:\n",
    "                self.grid[y][x] = self.curr_piece.color\n",
    "        if self.change_piece:\n",
    "            for pos in shape_pos:\n",
    "                p = (pos[0],pos[1])\n",
    "                self.locked_blocks[p] = self.curr_piece.color\n",
    "            self.curr_piece = self.next_piece\n",
    "            self.next_piece = get_shape()\n",
    "            self.change_piece = False\n",
    "            increment, new_block_set = clear_rows(self.grid, self.locked_blocks)\n",
    "            self.score += increment * 10\n",
    "            self.locked_blocks = new_block_set\n",
    "\n",
    "        draw_window(self.win, self.grid, self.score)\n",
    "        draw_next_shape(self.win, self.next_piece)\n",
    "        pygame.display.update()\n",
    "\n",
    "        self._state = 1\n",
    "        if check_lost(self.locked_blocks):\n",
    "#             draw_text_middle(win, \"You Lost!\", 80, (255, 255, 255))\n",
    "            self._state = 2\n",
    "            pygame.display.update()\n",
    "#             pygame.time.delay(end_wait)\n",
    "#             run = False\n",
    "            self.score = add_score(self.score)\n",
    "            self._episode_ended = True\n",
    "        if self._episode_ended:\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), self.score)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "                np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)\n",
    "#     return win, score, grid, locked_blocks, curr_piece, next_piece, check_lost\n",
    "#     if self._episode_ended or self._state >= 21:\n",
    "#       reward = self._state - 21 if self._state <= 21 else -21\n",
    "#       return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "#     else:\n",
    "#       return ts.transition(\n",
    "#           np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = np.array(0, dtype=np.int32)  # test with do nothing\n",
    "# time_step = environment.reset()\n",
    "# print(time_step)\n",
    "# while not time_step.is_last():\n",
    "#   time_step = environment.step(action)\n",
    "#   print(time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "environment = Tetris()\n",
    "# utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(2, dtype=int32), reward=array(0., dtype=float32), discount=array(0., dtype=float32), observation=array([2], dtype=int32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(0., dtype=float32), discount=array(0., dtype=float32), observation=array([2], dtype=int32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(0., dtype=float32), discount=array(0., dtype=float32), observation=array([2], dtype=int32))\n",
      "Final Reward =  0.0\n"
     ]
    }
   ],
   "source": [
    "get_new_card_action = np.array(0, dtype=np.int32)\n",
    "end_round_action = np.array(2, dtype=np.int32)\n",
    "\n",
    "# environment = Tetris()\n",
    "time_step = environment.reset()\n",
    "# print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "\n",
    "for episode in range (3):\n",
    "    time_step = environment.reset()\n",
    "    done = False\n",
    "    # for _ in range(20):\n",
    "    while not done:\n",
    "        time_step = environment.step(get_new_card_action)\n",
    "        #     print(time_step)\n",
    "        cumulative_reward += time_step.reward\n",
    "        if time_step.step_type == 2:\n",
    "            print(time_step)\n",
    "            done = True\n",
    "            time_step = environment.step(end_round_action)\n",
    "            \n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\\n\")\n",
    "            with open('data/score_log','a') as f:\n",
    "                f.writelines(str(episode+1)+','+str(int(time_step.reward))+','+dt_string)\n",
    "#         return score\n",
    "\n",
    "\n",
    "#comment out the following line to display last result\n",
    "pygame.display.quit()\n",
    "\n",
    "# print(time_step)\n",
    "cumulative_reward += time_step.reward\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__ (self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self_n_actions)\n",
    "        self.optimize = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = F.relu(self.fc3(x))\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-a2ee0be087bf>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-a2ee0be087bf>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def __init__ (self, gamma, epsilon, lr, input_dims[], batch_size, n_actions,\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__ (self, gamma, epsilon, lr, input_dims, batch_size, n_actions, \n",
    "                  max_mem_size=100000, eps_end = .01, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.action_space = [i for i in range (n_actions)]\n",
    "        self.max_mem_size = max_mem_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        self.Q_eval = DeepQNetwork(self.lr, n_actions = n_actions, input_dims = input_dims,\n",
    "                                  fc1_dims=256, fc2_dims=256)\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        indx = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[indx] = state\n",
    "        self.new_state_memory[indx] = state_\n",
    "        self.action_memory[indx] = action\n",
    "        self.reward_memory[indx] = reward\n",
    "        self.terminal_memory[indx] = done\n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval_forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.mem_cntr < sel.batch_size:\n",
    "            return\n",
    "        self.Q_eval.optimizer.zero.grad()\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace = False)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon>self.eps_min else self.eps_min\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-16-b98d5737b87f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-b98d5737b87f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    agent = Agent(gamma =.99, epsilon = 1.0, batch_size=50, n_actions=5, eps_end = .01, input_dims[blocks_w][blocks_h], lr =.003)\u001b[0m\n\u001b[0m                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = Tetris()\n",
    "    agent = Agent(gamma =.99, epsilon = 1.0, batch_size=50, n_actions=5, eps_end = .01, input_dims[blocks_w][blocks_h], lr =.003)\n",
    "    scores, eps_history = [], []\n",
    "    n_games = 500\n",
    "    \n",
    "    for i in range (n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score = reward\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            agent.learn()\n",
    "            observation = onbservation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        \n",
    "        print('episode', i, 'score %.2f' % score,\n",
    "             'average score %.2f' % avg_score,\n",
    "             'epsilon %.2f' % agent.epsilon)\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    filename = 'data/pytouch_img.png'\n",
    "    plot_learning_curve(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b0a5e074f4df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.number_of_actions = 2\n",
    "        self.gamma = 0.99\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.initial_epsilon = 0.1\n",
    "        self.number_of_iterations = 2000000\n",
    "        self.replay_memory_size = 10000\n",
    "        self.minibatch_size = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 4)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.fc4 = nn.Linear(3136, 512)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.fc5 = nn.Linear(512, self.number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# create the game\n",
    "game_state = Tetris()\n",
    "\n",
    "replay_memory = []\n",
    "\n",
    "\n",
    "while iteration < model.number_of_iterations:\n",
    "    # get output from the neural network\n",
    "    output = model(state)[0]\n",
    "\n",
    "    # initialize action\n",
    "    action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
    "    if torch.cuda.is_available():  # put on GPU if CUDA is available\n",
    "        action = action.cuda()\n",
    "\n",
    "    # epsilon greedy exploration\n",
    "    random_action = random.random() <= epsilon\n",
    "    if random_action:\n",
    "        print(\"Performed random action!\")\n",
    "    action_index = [torch.randint(model.number_of_actions, torch.Size([]), dtype=torch.int)\n",
    "                    if random_action\n",
    "                    else torch.argmax(output)][0]\n",
    "\n",
    "    if torch.cuda.is_available():  # put on GPU if CUDA is available\n",
    "        action_index = action_index.cuda()\n",
    "\n",
    "    action[action_index] = 1\n",
    "\n",
    "    # get next state and reward\n",
    "    image_data_1, reward, terminal = game_state.frame_step(action)\n",
    "    image_data_1 = resize_and_bgr2gray(image_data_1)\n",
    "    image_data_1 = image_to_tensor(image_data_1)\n",
    "    state_1 = torch.cat((state.squeeze(0)[1:, :, :], image_data_1)).unsqueeze(0)\n",
    "\n",
    "    action = action.unsqueeze(0)\n",
    "    reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)\n",
    "\n",
    "    # save transition to replay memory\n",
    "    replay_memory.append((state, action, reward, state_1, terminal))\n",
    "\n",
    "    # if replay memory is full, remove the oldest transition\n",
    "    if len(replay_memory) > model.replay_memory_size:\n",
    "        replay_memory.pop(0)\n",
    "\n",
    "    # epsilon annealing\n",
    "    epsilon = epsilon_decrements[iteration]\n",
    "\n",
    "    # sample random minibatch\n",
    "    minibatch = random.sample(replay_memory, min(len(replay_memory), model.minibatch_size))\n",
    "\n",
    "    # unpack minibatch\n",
    "    state_batch = torch.cat(tuple(d[0] for d in minibatch))\n",
    "    action_batch = torch.cat(tuple(d[1] for d in minibatch))\n",
    "    reward_batch = torch.cat(tuple(d[2] for d in minibatch))\n",
    "    state_1_batch = torch.cat(tuple(d[3] for d in minibatch))\n",
    "\n",
    "    if torch.cuda.is_available():  # put on GPU if CUDA is available\n",
    "        state_batch = state_batch.cuda()\n",
    "        action_batch = action_batch.cuda()\n",
    "        reward_batch = reward_batch.cuda()\n",
    "        state_1_batch = state_1_batch.cuda()\n",
    "\n",
    "    # get output for the next state\n",
    "    output_1_batch = model(state_1_batch)\n",
    "\n",
    "    # set y_j to r_j for terminal state, otherwise to r_j + gamma*max(Q)\n",
    "    y_batch = torch.cat(tuple(reward_batch[i] if minibatch[i][4]\n",
    "                              else reward_batch[i] + model.gamma * torch.max(output_1_batch[i])\n",
    "                              for i in range(len(minibatch))))\n",
    "\n",
    "    # extract Q-value\n",
    "    q_value = torch.sum(model(state_batch) * action_batch, dim=1)\n",
    "\n",
    "    # PyTorch accumulates gradients by default, so they need to be reset in each pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # returns a new Tensor, detached from the current graph, the result will never require gradient\n",
    "    y_batch = y_batch.detach()\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(q_value, y_batch)\n",
    "\n",
    "    # do backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # set state to be state_1\n",
    "    state = state_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.display.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
